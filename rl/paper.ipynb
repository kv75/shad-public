{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Cooling Optimization for Green Data Center via Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yuanlong Li, Yonggang Wen, Kyle Guan, and Dacheng Tao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1709.05077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы хотят решить задачу экономии электроэнергии в рамках отдельно взятого датацентра (или совокупности датацентов). Более конкретно, стоит задача оптимизации расхода электроэнергии в системе охлаждения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть классические подходы к решению этой задачи. Они основаны на построении физической модели процессов, происходящих в датацентре, с использованием законов термодинамики, электричества и механики. Проблемы: сложность моделирования и решения. Кроме того, для каждого нового датацентра нужно строить новую модель и решать её."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы предлагают решать задачу методами ML, а точнее, RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Состояние:\n",
    "* $T_{amb}(t)$ — внешняя температура;\n",
    "* $H(t)$ — количество теплоты, выделяемое источниками тепла в датацентре (оборудование, освещение, персонал)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действия — изменение параметров системы охлаждения. В модельной задаче — установка значений температуры $5$ охлаждающих устройств."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Награда — функция параметров\n",
    "* $\\epsilon$ — потреблённая энергия;\n",
    "* $T_z$ — температура в датацентре.\n",
    "\n",
    "$$\n",
    "r = -y\\\\\n",
    "y = \\epsilon + \\lambda \\ln (1 + \\exp(T_z - \\phi))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q = r,$ так как авторы положили $\\gamma = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методика: off-policy алгоритм, адаптированный вариант <a href=\"https://spinningup.openai.com/en/latest/algorithms/ddpg.html\">Deep Deterministic Policy Gradient</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Две нейронные сети:\n",
    "* $Q$ network — предсказывает награду как функцию состояния и действия;\n",
    "* $\\mu$ network (policy network) — выбирает действие в зависимости от текущего состояния и, возможно, истории.\n",
    "\n",
    "![Архитектура нейронных сетей](fig3.png \"Архитектура нейронных сетей\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм\n",
    "* to be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
